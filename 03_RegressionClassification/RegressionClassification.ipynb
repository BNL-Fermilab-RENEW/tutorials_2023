{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299f3af-0e57-4900-a83f-7f88c647a86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151f636-479a-40b0-b118-ffcd8c6ba9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcfafb-ad8c-4ef2-bbca-6e6283dec613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f592eb-ad45-414f-acad-d6e2a51a5f26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae89be-1442-43a9-aa9a-2cbff70f4ce5",
   "metadata": {},
   "source": [
    "Written by: [Matthew R. Carbone](https://www.bnl.gov/staff/mcarbone) | _Assistant Computational Scientist, Computational Science Initiative, Brookhaven National Laboratory_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eaf124-eb7b-41c7-b4f1-6792fd8fec3b",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to go over the fundamentals of regression and classification, which are the two most common types of supervised learning. We will also discuss some of the best practices in machine learning, such as a train-validation-testing split. In regression problems, the objective is to learn a _continuous_ output. In classification problems, the objective is to learn a _discrete_ output. Here are some examples:\n",
    "- Predicting the cost of a house from its properties, such as square footage, number of bathrooms, etc. is a _regression_ problem.\n",
    "- Whether or not an image is of a cat or dog is a _classification_ problem.\n",
    "- Predicting the type of animal in an image is a _classification_ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df42443-4fbb-47a6-bb3a-b96c92f5e675",
   "metadata": {},
   "source": [
    "**Learning objectives:**\n",
    "- Understand a variety of regression and classification algorithms.\n",
    "- Start to explore some of the fundamental concepts in machine learning, such as splitting data, overfitting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132cd22-31cc-4bb0-8823-a6d6c2e4704a",
   "metadata": {},
   "source": [
    "Regression and classification problems can be solved via a variety of different methods (or _models_). In this tutorial, we're going to go over the following types of models, which will form the backbone of your understanding for e.g. neural networks, and other types of machine learning, later on.\n",
    "- Linear regression\n",
    "- Polynomial regression\n",
    "- Logistic regression\n",
    "\n",
    "There are numerous [other types of models](https://www.listendata.com/2018/03/regression-analysis.html#Linear-Regression) which we simply won't have the time to dive into, but paradigmatically, the objective of all of these models is the same. Given some input, predict some output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ab799-d376-4921-b58d-f9f9330ed9cc",
   "metadata": {},
   "source": [
    "## Ingredients for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bfca4-511b-4233-b141-f390c5079fc4",
   "metadata": {},
   "source": [
    "There are a few \"ingredients\" to always consider when approaching a regression problem.\n",
    "- Your available data (\"dataset\")\n",
    "- Your choice of model (\"model\")\n",
    "- How you choose to fit the model to the data (\"optimizer\")\n",
    "- An indicator for how well your model fits the data (\"loss function\"/\"metric\"/\"criterion\")\n",
    "\n",
    "We will discuss all of these components today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace206c5-00d1-4175-9ac3-23b7ee8a18e4",
   "metadata": {},
   "source": [
    "## Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edaf978-57f3-46fb-9824-432381d64793",
   "metadata": {},
   "source": [
    "- [Andrew Ng's flagship Coursera course on machine learning](https://www.coursera.org/specializations/machine-learning-introduction)\n",
    "- [Intro to regression analysis](https://towardsdatascience.com/introduction-to-regression-analysis-9151d8ac14b3)\n",
    "- [15 types of regression](https://www.listendata.com/2018/03/regression-analysis.html#Linear-Regression)\n",
    "- [Gradient descent tutorial](https://machinelearningmind.com/2019/10/06/gradient-descent-introduction-and-implementation-in-python/)\n",
    "- [Linear regression gradient descent tutorial](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177bded-5e38-4c6b-a71e-52de42f930ee",
   "metadata": {},
   "source": [
    "# Linear regression and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec192d-61b3-4e78-aa70-e11392a8eebe",
   "metadata": {},
   "source": [
    "Let's begin with the simplest form of regression: that of fitting a line to data. We can recast this problem as learning a function $f(x) = y,$ where the form of $f$ is simply the familiar $f(x) = mx + b.$ Given a dataset $\\{x_i, y_i\\}$, we can \"learn\" the coefficients $m$ and $b$ that best model the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace82bd-f62b-4c7b-b424-502780c3795c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, b):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d047f-0326-447c-a77d-876720ff1697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_data_with_noise(seed=123, scale=0.5, N=100, slope=2.4, y_intercept=0.8):\n",
    "    np.random.seed(seed)\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = linear_model(x, slope, y_intercept) + np.random.normal(scale=scale, size=(N,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0075f0-3b09-4d0e-9969-2b3669baafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = linear_data_with_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfacd60-7e84-4ee5-9fca-486dc5a05990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30ce42-0f9e-4595-93c3-c8e7a905a590",
   "metadata": {},
   "source": [
    "Suppose we fix $m$ and $b$ to some values $m_0$ and $b_0$. We now have a model which is completely defined, and given some value for $x$, we can predict $y.$ But how do we know if these are good choices? We need to define a metric, or a measure of how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012648e-cbd8-4d5f-93ee-58d4b52052a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def criterion(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f98e2-bea8-44b6-a764-06e93989d155",
   "metadata": {},
   "source": [
    "Above, our `criterion` is called the average mean squared error. The square root of this is the \"root-mean-squared\" error, which you are likely familiar with. For the purposes of this example, it doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630f25d-04a4-4354-80f4-dee6e36fae50",
   "metadata": {},
   "source": [
    "## Evaluating an arbitrary model using a \"random\" optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5ba7c-598d-4d5d-872f-4bdbd6dd0588",
   "metadata": {},
   "source": [
    "We know what the ground truth slope `m` and y-intercept `b` actually are, but let's pretend we don't, and evaluate a set of linear models against the data that we have. We can take a bunch of values for `m0` (guesses for the slope) and `b0` (guesses for the y-intercept), create a linear model with those parameters, and evaluate those against the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41041931-7a83-48a8-89e1-8fd00da8de66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "m0 = np.random.random(size=100) * 6 - 3  # 100 random numbers between -3 and 3\n",
    "b0 = np.random.random(size=100)          # 100 random numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35b064-0388-473e-b7fc-7eef131bb705",
   "metadata": {},
   "source": [
    "Let's try every one of these combinations for `m0` and `b0`, and valuate the model against our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de26ed2-08b6-4303-b512-1be843f1837c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ec25a-2c68-4164-a54e-bedeb0626e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m0b0 = list(product(m0, b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3011f10-4eb2-4ee5-914d-6ddb994874a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"m0\": [params[0] for params in m0b0],\n",
    "    \"b0\": [params[1] for params in m0b0],\n",
    "    \"criterion\": [criterion(y, linear_model(x, *params)) for params in m0b0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401523c-ece0-448e-b5ff-a65eda1ab69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746c23e-7fd0-4477-b073-dadff00f85ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "argmin = df[\"criterion\"].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02cd92-a4c4-42a3-baec-85dc446b985c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[argmin, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d7ecc-4798-4ef0-90b8-8d9bf5b92df6",
   "metadata": {},
   "source": [
    "## ⚠️ Check your understanding/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f64c98-7784-43a3-88b5-f98fccf243d8",
   "metadata": {},
   "source": [
    "What happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf7150-c710-481b-962e-f9618c0d3109",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A smarter optimizer: gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeab12c-e4a6-460c-aa8a-baa12aa5c76d",
   "metadata": {},
   "source": [
    "It stands to reason that just randomly guessing parameters is probably not the most effective way of finding the right guess. Can we do better? Absolutely! [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), and its family of related methods (you may have heard of [Adam](https://arxiv.org/abs/1412.6980)), can be used to find an optimal set of parameters for arbitrary models.\n",
    "\n",
    "Gradient descent, and its related methods, are all about finding the optima of functions. For example, if I give you a function $f(x) = (x-2)^2$ and ask you to find its minimum, how would you do this. From calculus, you might know how. You also might recognize this is a simple parabola and know the answer right away. But what about some arbitrary function, possibly with many _local_ minima, and possibly a _huge_ number of parameters (in machine learning, this number can easily be greater than $10^7$)? Gradient descent is a method for finding minima numerically.\n",
    "\n",
    "Given a function $f(x)$ and its derivative $\\nabla f(x),$ the general rule of gradient descent is that to find a point closer to the actual minimum of $f,$ we follow its gradient:\n",
    "\n",
    "$$x_{n+1} = x_n - \\gamma \\nabla f(x_n).$$\n",
    "\n",
    "This is perhaps easiest to see through an example. Let's consider the function $f(x) = (x-2)^2,$ which of course has derivative $df(x)/dx = 2(x-2).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca78118-a5ef-4cfc-8ace-9096c71d287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f_prime, gamma, N=100):\n",
    "    \"\"\"The gradient descent algorithm for a scalar function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : float\n",
    "        The initial guess for the minimum.\n",
    "    f_prime : callable\n",
    "        The derivative of the function of interest.\n",
    "    gamma : float\n",
    "        Learning rate.\n",
    "    N : int\n",
    "        Number of iterations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the values of x found during gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = [x0]\n",
    "    for ii in range(N):\n",
    "        x_n = res[-1]\n",
    "        res.append(x_n - gamma * f_prime(x_n))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f87224-49ce-4561-b700-0dac55129be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = np.array(gradient_descent(\n",
    "    x0=10.0,\n",
    "    f_prime=lambda x: 2.0 * (x - 2.0),\n",
    "    gamma=0.1,\n",
    "    N=100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2d217-0f10-4e61-b7c3-54f1689a9138",
   "metadata": {},
   "source": [
    "We of course know that the true minimum of this function is at $x=2,$ but lets plot how well the `gradient_descent` function finds this minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a98ee-8b4f-483e-88da-d0030747c179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps[\"viridis\"].resampled(len(points))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "xgrid = np.linspace(-1, 15, 100)\n",
    "ax.plot(xgrid, (xgrid - 2.0)**2, \"k-\", label=r\"$f(x)=(x-2)^2$\")\n",
    "ax.scatter(points, (points - 2.0)**2, c=[cmap(ii) for ii in range(len(points))], s=10)\n",
    "ax.axvline(2.0, zorder=-1, color=\"red\", label=\"min $f(x)$\")\n",
    "ax.legend(frameon=False, fontsize=5)\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$f(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a8709-11ad-4078-a9ca-4a0dbd70789f",
   "metadata": {},
   "source": [
    "## ⚠️ Check your understanding/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9f161-69dc-4913-a631-fcdedba19dfa",
   "metadata": {},
   "source": [
    "Play around with the above code a bit. Here are some examples of things you can try.\n",
    "- What happens if you make $\\gamma$ really large?\n",
    "- What happens if you make $\\gamma$ really small?\n",
    "- What happens if you change $x_0$?\n",
    "- What happens if you change the function? For example, try $f(x) = \\sin x$ (remember the derivative from calculus?). How sensitive is this procedure to your choice of $x_0$?\n",
    "\n",
    "Here's a [video](https://upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.720p.vp9.webm) of what gradient descent is doing with a lot of points. This arbitrary function is two dimensional. If you are not sure what's happening here, ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f784c3-bd5a-4b50-ad43-79e138b37f32",
   "metadata": {},
   "source": [
    "## Minimizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe7ca3-01b0-4006-a7c0-c1e82aad173a",
   "metadata": {},
   "source": [
    "Now we put two pieces together. As a reminder, the criterion, which we will now refer to as a loss function, is a measure of how different two data points are. We wish to minimize the average of this loss function when doing machine learning. \n",
    "\n",
    "For example, consider the linear fit from before. Our ground truth \"training data\" is $\\{x_i, y_i\\},$ and our model is simply $f(x; m, b) = mx + b$. We want to _minimize_ the difference between the ground truth values and predicted values on our dataset:\n",
    "\n",
    "$$ L(m, b) = \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i; m, b))^2 $$\n",
    "\n",
    "Specifically, we want to minimize $L$ with respect to $m$ and $b.$ Stated slightly differently, we want to find the corresponding values of $m, b$ such that $L$ is minimized. We just learned of a way to do this, gradient descent! However, to use gradient descent, we need the partial derivatives of $L$ with respect to all parameters of interest. Luckily, $L$ is differentiable analytically.\n",
    "\n",
    "NOTE: ALL machine learning models trained with the family of gradient descent methods have to have analytically known derivatives. This includes neural networks!\n",
    "\n",
    "The derivatives are:\n",
    "\n",
    "$$ \\frac{\\partial L(m, b)}{\\partial m} = -\\frac{2}{N}\\sum_{i=1}^N (y_i - f(x_i; m, b)) x_i$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial L(m, b)}{\\partial b} = -\\frac{2}{N}\\sum_{i=1}^N (y_i - f(x_i; m, b)).$$\n",
    "\n",
    "Following the rule from before, the update rules for $m$ and $b$ are:\n",
    "\n",
    "$$ m_{n+1} = m_n - \\gamma \\frac{\\partial L(m, b)}{\\partial m}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ b_{n+1} = b_n - \\gamma \\frac{\\partial L(m, b)}{\\partial b}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f37f3-a06f-4922-baae-a0d111c711a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_linear_model(x, y, m0, b0, gamma=0.1, N=100):\n",
    "    b = [b0]\n",
    "    m = [m0]\n",
    "\n",
    "    for _ in range(N):\n",
    "        \n",
    "        y_pred = linear_model(x, m[-1], b[-1])\n",
    "        \n",
    "        # Calculate the derivatives of the loss function\n",
    "        dLdm = -2.0 / len(x) * np.sum(x * (y - y_pred))\n",
    "        dLdb = -2.0 / len(x) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update\n",
    "        m.append(m[-1] - gamma * dLdm)\n",
    "        b.append(b[-1] - gamma * dLdb)\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13311f6-184f-4038-b818-993ab6ddcdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, b = gradient_descent_linear_model(x, y, m0=-1.9, b0=-5.0, gamma=0.1, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dbc94-d9bf-445d-8f3e-85c70cdb53ce",
   "metadata": {},
   "source": [
    "If we plot the convergence of `m` and `b` as a function of \"epoch\" (or number of full passes through the training data) we can see a very fast convergence of the parameters to the true parameters, which are $m = 0.8$ and $b=2.4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957690ef-36e6-4988-af78-9db493e9e9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "ax.axhline(0.8, color=\"black\", linewidth=0.5)\n",
    "ax.axhline(2.4, color=\"black\", linewidth=0.5)\n",
    "\n",
    "ax.plot(m, \"ro-\", linewidth=0, markersize=0.5, label=\"$m$\")\n",
    "ax.plot(b, \"bo-\", linewidth=0, markersize=0.5, label=\"$b$\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4b3e5-56ae-4a2b-a67d-285f5ede2881",
   "metadata": {},
   "source": [
    "We can also plot the linear fit itself as a function of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217987a-c9c8-478a-8a78-752396f31818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps[\"viridis\"].resampled(len(m))\n",
    "grid = np.linspace(-1, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "\n",
    "\n",
    "for ii, (mm, bb) in enumerate(zip(m, b)):\n",
    "    ax.plot(grid, linear_model(grid, mm, bb), color=cmap(ii))\n",
    "    \n",
    "ax.scatter(x, y, zorder=1)\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f1df8-a7e2-48d5-9a49-ed2e281a3e52",
   "metadata": {},
   "source": [
    "## ⚠️ Check your understanding/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79799570-5536-4541-b182-6a8ce30b9391",
   "metadata": {},
   "source": [
    "As with the previous section, understanding what's going on here is critical. Once again, ask and answer (discuss with your neighbors), what is going on here? **If you can understand this simple example, you will be able to understand all of gradient-based machine learning that we present as part of this tutorial series!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb0c66-cbfc-4617-aa35-916c21867148",
   "metadata": {},
   "source": [
    "## California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1a992-0846-46c4-839c-b6abefecbb7d",
   "metadata": {},
   "source": [
    "Let's apply the knowledge we've gained from the previous sections on a \"real\" problem. We'll be using the [California Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), which is a classic teaching example in machine learning. We'll also loosely follow along with this [tutorial](https://towardsdatascience.com/introduction-to-regression-analysis-9151d8ac14b3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcce43-8ac7-467f-b16b-211f37962b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a017a7-eab1-4019-b62d-2d99c66c64f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "\n",
    "# Put the data into a cleaner format:\n",
    "df = pd.DataFrame({key: data[\"data\"].T[ii] for ii, key in enumerate(data[\"feature_names\"])})\n",
    "df[data[\"target_names\"][0]] = data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbcaea-f8be-4120-9ba3-221b0000073a",
   "metadata": {},
   "source": [
    "Before we fit anything, let's do some exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab39e93-09b5-4c6c-a42d-aa83caf2f493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"MedHouseVal\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43577c9-2aa4-4daa-8b30-f444675edeed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"MedHouseVal\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ec9b7-b4b7-44de-be7a-f9c68b5e73aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3059c-fa7d-4fd9-a048-5862b59acc17",
   "metadata": {},
   "source": [
    "The strongest correlation between our inputs and outputs comes from the `HouseAge`, `AveRooms` and especially `MedInc`. We'll use these three variables as part of a multilinear regression model:\n",
    "\n",
    "$$ f(x_1, x_2, x_3) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_3, $$\n",
    "\n",
    "where $x_1, x_2, x_3$ are the values of the three input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32264948-2225-47a4-a844-13d948e8ab53",
   "metadata": {},
   "source": [
    "Although there is not much linear correlation between latitude and longitude and our target, we can still make a pretty plot. Kinda looks like California!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875d01a-fb1c-413f-9aed-a8dbabd3e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "\n",
    "\n",
    "ax.scatter(df['Latitude'], df['Longitude'], c=df['MedHouseVal'], s=df['Population']/1000)\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(), ax=ax)\n",
    "ax.set_xlabel(\"Latitude\")\n",
    "ax.set_ylabel(\"Longitude\")\n",
    "cbar.set_label(\"MedHouseVal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6485fc9-6a2e-43fa-9b85-99c999d2678c",
   "metadata": {},
   "source": [
    "## ⚠️ Check your understanding/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2dd89-6d15-4258-9199-43fff6d2694e",
   "metadata": {},
   "source": [
    "Train a multilinear regression model using gradient descent on the California Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2deb3-3063-4ff7-bdbb-86e11f457954",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d7715-8146-45ef-a238-c820253011e3",
   "metadata": {},
   "source": [
    "In machine learning, and in any data-driven modeling, we have to \"fit\" the model on some dataset, but then we want an unbiased estimation of how the model will perform on data it has not yet seen before. We can easily do this with the `sci-kit learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21020ced-6af9-4e3f-9725-ab2e42feb128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"HouseAge\", \"AveRooms\", \"MedInc\"]].to_numpy()\n",
    "y = df[\"MedHouseVal\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457563bf-99ff-4ba0-a8ba-ab33c5c596f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695b3ea-4400-4cee-88c8-3c669b3d8f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616beab-2fe8-4027-9cde-40f443b30585",
   "metadata": {},
   "source": [
    "Now let's train a linear model. Since you've already done this the hard way, now we can do it the easy way (using existing libraries)! It's also relevant to note that unlike in most machine learning problems (which use non-linear models), linear models can be fit analytically (and quickly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde431fa-5d6b-4fc8-941c-269e98b7c4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87e768-3fd8-4124-b710-6e6448fffbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9c344-4c53-4926-8441-83328aaeed3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18d457-922b-4f4e-aad3-623c8184c577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b6c06-4388-4324-a0e2-208969c13b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682862af-3768-43d5-820c-cc40c541ac67",
   "metadata": {},
   "source": [
    "As before, we want to quantify the degree of fit. We can once again use the criterion to determine the quality of the fit. Let's do this on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8b202-2822-42ca-a635-80ecd074bffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54889035-e309-4662-9b59-85c395ddd3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a5617-0d39-4f38-8d2f-a2ad78131d7f",
   "metadata": {},
   "source": [
    "Given that the range of the target is roughly 5, this is not a terrible model. We can likely do better by including more variables, or by making our model more complex (which we will explore in future tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5799900-0acd-482e-9b50-0adcff693828",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b97fa7-324b-49d3-be75-ce65eebd160c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24793c5-bf7c-4607-ab64-9fba47660048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
