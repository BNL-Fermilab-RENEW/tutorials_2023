{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX_9BuPB_hA-"
      },
      "source": [
        "# Day 5: Introduction to Deep Neural Networks!\n",
        "\n",
        "In this tutorial, we're going to go over the fundimentals of Neural Networks, specifically focusing on **Dense Neural Networks**. (If you're interested in Convolutional Neural Networks, don't fret! You'll probably enjoy the next lecture ;) )\n",
        "\n",
        "Neural Networks as a whole are an _incredibly_ broad and complex topic. As we only have half a day today, we're not going to be able to cover much more than the basic ideas and techniques, but the hope is that this provides a stable base for you to continue building your knowledge on top of. If you don't itend to continue much farther down the machine learning rabbit hole, we hope that at the very least, this can serve to de-mystify neural networks/machine learning as a whole :)\n",
        "\n",
        "**Learning Objectives:**\n",
        "* Understand Vectors, Matricies, and Dot Product\n",
        "* Understand how a single layer perceptron works, along with batching and gradient descent.\n",
        "* Learn how to build and train a simple Deep Neural Network (Multi-Layer Perceptron - MLP) using the TensorFlow/Keras deep learning python framework\n",
        "* Learn about different optimizers beyond (Stochastic) Gradient Descent and touch on when you might want to use them\n",
        "* Touch on a few key \"Hyperparameters\" that you, the human, can and will need to optimize when designing a neural network (Learning Rate, Batch Size, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o65_IgEgLtT2"
      },
      "outputs": [],
      "source": [
        "# Run Me Ahead of time! Need to download some data :)\n",
        "!pip install numpy sklearn matplotlib\n",
        "!pip install tensorflow\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "jet_tagging_data = fetch_openml('hls4ml_lhc_jets_hlf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJdBSPiRLtT3",
        "tags": []
      },
      "source": [
        "## Notation, Terms, and other background info\n",
        "In this notebook, you'll likely see some new symbols, terminology, and other topics that you haven't come across before. Don't be afraid!\n",
        "\n",
        "Most of the math symbols you see here simply function as *shorthand* for different math concepts, so that we can write an equation using these concepts without needing to define \"generic\" variables or functions every time we reference them. We've compiled a list of these terms here for you to reference when needed, but at least skimming through these before continuing is reccomended.\n",
        "\n",
        "Additionally, there are some math concepts that are useful to know about/have a basic understanding of, as they're used quite heavily in this notebook (and Neural Networks/Machine Learning in general!)\n",
        "\n",
        "We're going to assume that you have a baseline level of knowledge regarding some math concepts, but if you run into something that you don't understand (regardless of if it's defined here or not!), please let us know and we'll be happy to explain it! Additionally, some definitions of terms might include Machine Learning concepts that are covered later in the notebook (e.g. *Activation Functions*), we'll include another list of terms/symbols that are covered as part of this notebook at the end, so for now don't worry too much if you run into one of these terms.\n",
        "\n",
        "## Math Concepts\n",
        "\n",
        "###  Vectors & Scalars\n",
        "a **vector** is a kind of variable that not only has a value (aka *magnitude*), but also a *direction*. One common way to represent and define vectors that we'll use is by stating the change along each axis (the *components*) the vector travels in (in 2D, this would be $x$ and $y$ *components*, in 3D, the $x$, $y$ and $z$ *components* ).\n",
        "\n",
        "This can be written as $\\vec{v} = (x,y)$ _or_ as $\\boldsymbol{v} = (x,y)$ (note the second $v$ is bolded). Sometimes, the *components* are written stacked vertically such as $\\vec{v} = \\begin{pmatrix} x\\\\ y\\\\ \\end{pmatrix}$\n",
        "\n",
        "In this format, calculating the *magnitude* of a vector, written as $| \\vec{v} |$, is done by applying the pythagorean theorm!\n",
        "\n",
        "<img src=\"img/vector_components.png\" alt=\"Plot labeling the components of a vector, plus the definition for mangitude\" style=\"background-color:white; width:400px;\" />\n",
        "\n",
        "* You can add or subtract vectors to/from vectors, this will produce a vector as a result ( the *resultant*)\n",
        "* Multiplication of a Vector with a Scalar produces a vector\n",
        "* Multiplication of Vectors with Vectors can produce either:\n",
        "    * Another Vector, by calculating the *Cross Product*: $\\vec{v} \\times \\vec{b} = \\vec{vb}$\n",
        "    * a *Scalar*, by calculating the *Dot Product*: $\\vec{v} \\cdot \\vec{b} = vb$\n",
        "* You cannot divide with vectors\n",
        "    \n",
        "a **scalar** is a normal value, without a direction. Scalars are considered to only have a *magnitude*. Most math you know and have done throughout school has been **scalar** math :)\n",
        "\n",
        "\n",
        "### Matrix/Matrices\n",
        "a **Matrix** is a _structured_ table/an array of numbers. Location of each element matters, and they can be **1 or more dimensions** in shape. Matricies are generally labeled as capital/uppercase letters, and are represented with it's elements placed in a rectangular grid, encapsulated by large square brackets $[  ]$ or parenthesis $(  )$  A Matrix with $m$ rows and $n$ columns can be written as    \n",
        "$$\n",
        "A = [a_{m,n}] = \\begin{bmatrix}\n",
        "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\\n",
        "  \\vdots & \\ddots & \\vdots\\\\\n",
        "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "  {a}_{1,1} & \\dots & {a}_{1,n}\\\\\n",
        "  \\vdots & \\ddots & \\vdots\\\\\n",
        "  {a}_{m,1} & \\dots & {a}_{m,n}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "So if we set $m=4$ and $n=2$\n",
        "$$\n",
        "A = [a_{4,2}] = \\begin{bmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Where ${a}_{i,j}$ (no brackets/parentheses) refers to a single element in the matrix, where $i$ refers to the row and $j$ refers to the column\n",
        "\n",
        "* Sometimes, we'll talk about *transposing* a matrix, which means we flip it along it's diagonal axis! When transposing a matrix, we swap the values of $m$ and $n$ and move the elements of the matrix to match. We refer to the transposed version of a matrix by adding a superscript of $T$ to it. For example\n",
        "<center><img src=\"img/Matrix_transpose.gif\" alt=\"Animation of a matrix being transposed\"/></center>\n",
        "$$\n",
        "A = [a_{3,2}] = \\begin{bmatrix}\n",
        "  1 & 2\\\\\n",
        "  3 & 4\\\\\n",
        "  5 & 6\\\\  \n",
        "\\end{bmatrix} \\quad  A^{T} = [a^{T}_{2,3}] = \\begin{bmatrix}\n",
        "  1 & 3 & 5 \\\\\n",
        "  2 & 4 & 6 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "* A 1-Dimensional matrix (where either $n=1$ or $m=1$) can also be called a **row vector** (when shape = $m \\times 1$) or a **column vector** (when shape = $1 \\times n$), and transposing one kind turns it into the other. These can be _treated_ like vectors (such as when you're performing certain operations), but are still matricies. A key point is that a *vector* **can not** be transposed, but a *matrix* can. (Yes this is confusing, this distinction won't come up today, but is important with Linear Algebra in general)\n",
        "$$\n",
        "A = [a_{4,1}] = \\begin{bmatrix}\n",
        "  1 \\\\\n",
        "  2 \\\\\n",
        "  3 \\\\\n",
        "  4 \\\\\n",
        "  \\end{bmatrix} \\quad B = [b_{1,4}] = \\begin{bmatrix}\n",
        "  5 & 6 & 7 & 8\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* **Importantly,** you can/will see some matricies represented in the form of a *column vector*, functioning as a ***table*** of vectors. You have to be careful in this distinction, as it's shorthand that needs to be expanded before performing operations on it as a matrix. For example, if we have a matrix $A = [a_{4,3}]$, we can write it as a table of 2-Dimensional vectors $\\vec{a_n} = (x,y)$ like so:\n",
        "\n",
        "$$\n",
        "A = [a_{4,2}] = \\begin{bmatrix}\n",
        "  \\vec{a_1} \\\\\n",
        "  \\vec{a_2} \\\\\n",
        "  \\vec{a_3} \\\\\n",
        "  \\vec{a_4} \\\\\n",
        "  \\end{bmatrix} = \\begin{bmatrix}\n",
        "  {a_1}_x & {a_1}_y\\\\\n",
        "  {a_2}_x & {a_2}_y\\\\\n",
        "  {a_3}_x & {a_3}_y\\\\\n",
        "  {a_4}_x & {a_4}_y\\\\\n",
        "  \\end{bmatrix} = \\begin{bmatrix}\n",
        "  {a}_{1,1} & {a}_{1,2}\\\\\n",
        "  {a}_{2,1} & {a}_{2,2}\\\\\n",
        "  {a}_{3,1} & {a}_{3,2}\\\\\n",
        "  {a}_{4,1} & {a}_{4,2}\\\\\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Dot Product\n",
        "The dot product of two matricies is simply the sum of each corresponding element multiplied together, defined as such:\n",
        "\n",
        "$ \\mathbf a \\cdot \\mathbf b = \\sum_{i=1}^n a_i b_i = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n $\n",
        "\n",
        "\n",
        "\n",
        "***Variable/Function Definitions - ML/Notebook Specific***\n",
        "Most of these symbols are common across Machine Learning/Neural Network literature, but there still might be slight variations. For the purposes of this notebook, these variables are defined as follows\n",
        "\n",
        "Dataset Related Variables:\n",
        "* $\\vec{x}$ - An **unmodified** *input* vector, these make up the inputs to our neural network\n",
        "* $\\vec{x_n}$ - pronounced \"*x n*\", e.g. \"x 1\", \"x 2\", etc. - a specific input matrix (feature), e.g. $\\vec{x_1}$, $\\vec{x_2}$, etc. (this also applies to other forms of $\\vec{x}$, $\\vec{y}$, and $\\vec{w}$ such as $\\vec{x'}$ and $\\vec{\\hat{y}}$ )\n",
        "* $\\vec{x'}$ - pronounced *\"x prime\"* - Generally a modified value of x (derivitive of, though not speciifcally in the Calculus sense :) ), usually referring to the output of an intermediete step during pre-processing, an operation within a *hidden layer* of a neural network, or the output of the *hidden layer* itself\n",
        "* $\\vec{y}$ - A truth vector, containing whatever the _actual truth values_ that the neural network is trying to learn to replicate.\n",
        "* $\\vec{\\hat{y}}$ - pronounced *Y Hat* - the **modified**/**final** output vector of our neural network, usually after we apply an *activation function*\n",
        "* ${X}$ - (Capital/Uppercase X) - the _input_ matrix for our neural network, comprised of all input vectors ($x$)\n",
        "* $\\hat{Y}$ - (Capital/Uppercase Y Hat) - the _output_ matrix for our neural network, comprised of all output vectors ($\\hat{y}$) from our neutral network.\n",
        "\n",
        "Neural Network Related Variables/Functions:\n",
        "* $\\vec{w}$ - a *weight* vector, a _learned parameter_ that a neural network learns during the training process.  \n",
        "* $b$ - a *bias* scalar, a _learned parameter_ that a neural network learns during the training process.\n",
        "* $g(...)$ - an *activation function*, used to add *non linear behavior* to the network. This can represent one of multiple different functions (such as Step, ReLU, Sigmoid, TanH, etc. see the \"Activation Functions\" link in *Other Resources* above!), depending on the context and architecture of a given neural network.\n",
        "\n",
        "* $Q(...)$ - a *loss function*, used to calculate how far off one varaible\n",
        "\n",
        "***Math Symbols/Definitions***\n",
        "This notation is pretty general and shared across most math literature you'll find, but it's important to check specific meanings regardless, **ESPECIALLY** if this notation is being used in other contexts/fields (Outside ML, Discrete Math, and Linear Algebra)\n",
        "* $\\in$ - pronounced as \"in\" - A symbol denoting that the value of a mentioned variable (usually $x$, $y$, etc.) exists within/is bounded by some set of numbers, meaning that it will _always_ be within that set and never be anything that's not part of it.\n",
        "* $\\{a, b, c\\}$ - pronounced \"set of ...\" - This is specificlly defined set (list) of numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol. (Note the curly braces $\\{ \\}$)\n",
        "* $\\mathbb{R}$ - pronounced \"\\<the set of> all real numbers\" - This symbol represents the set of **all** real numbers, usually used in conjunction with the above \"in\"/ $\\in$ symbol.\n",
        "* In this notebook, we also use the notation $\\mathbb{R}^d$ or $\\{a, b, c\\}^d$  , where $d$ indicates the dimensions/shape of whatever the variable we're referencing (Outside of this notebook, $\\mathbb{R}^n$ is often the same meaning where $d$ = $n$).\n",
        "    * e.g. $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ means that a ***matrix*** $\\hat{Y}$ in the shape of \"${4 \\times 1}$\", where each element of the matrix is a real number (part of the set $\\mathbb{R}$)\n",
        "    * e.g. $\\vec{x} \\in \\mathbb{R}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are all real numbers.\n",
        "    * e.g. $\\vec{x} \\in \\{0,1\\}^2$ is a ***vector*** $\\vec{x}$ in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  \n",
        "    * e.g. $\\vec{\\hat{y}} \\in \\{0,1\\}$ is a ***vector*** $\\vec{\\hat{y}}$ in $1$ dimensional space, where the possible values of the vector's *component* is either $0$ or $1$\n",
        "    * e.g. $\\{0,1\\}^2$ is a ***vector*** (that's unnamed) in $2$ dimensional space, where the possible values of the vector's *components* are either $0$ or $1$.  \n",
        "* $\\nabla$ - The Greek letter Nabla - This symbol denotes the _gradient_ of a given function, which is a vector that describes the direction that increases the fastest from that given point. It's coordinates are the _partial derivatives_ of that function at a given point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZuLT_rXLtT4"
      },
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the process black-boxed by deep learning frameworks.\n",
        "\n",
        "\n",
        "## Example Task: Boolean Logic Gates\n",
        "In order to focus on explaining the internals of training, this notebook uses a simple and classic example: *boolean logic gates* (aka *threshold logic units*).\n",
        "\n",
        "When we talk about boolean logic, we refer to operations that exclusively use ***Truth Values***, which are *binary* values that can **only** be *True* or *False*, typically represented where $x=0$ means *False* and $x=1$ means *True*.\n",
        "\n",
        "\n",
        "*Boolean Logic Gates* take one or more boolean values as input and produce a single boolean output. Some basic *Boolean Logic Gates* are:\n",
        "* AND ($\\wedge$) - Takes two inputs (A & B) and outputs *True* **if and only if** both A and B are True. Otherwise outputs *False*.\n",
        "* OR ($\\vee$) - Takes two inputs (A & B) and outputs *True* **if either** A or B are *True*. Otherwise outputs *False*.\n",
        "* NOT ($\\lnot$) - Takes one input (A) and *inverts* it. if A is True, it outputs *False*. If A is False, it outputs *True*.\n",
        "* NAND ($\\uparrow$) - Takes two inputs (A & B) and outputs *True* **if only both outputs are the not true* A and B are not. Otherwise outputs *False*.\n",
        "    * This is the opposite of the AND Gate, effectivly as if you take the output of an AND gate and pass it through a NOT gate\n",
        "* NOR ($\\downarrow$) - Takes two inputs (A & B) and outputs *True* **if** A or B are *False*. Otherwise outputs *False*.\n",
        "    * This is the opposite of the NOR Gate, effectivly as if you take the output of an OR gate and pass it through a NOT gate\n",
        "* XOR ($\\oplus$) - *exclusive OR* - Takes two inputs (A & B) and outputs *True* if **only one input* (A or B, but not both at the same time) is *True*. Otherwise outputs *False*.\n",
        "* XNOR ($\\odot$) - *exclusive NOR* - Takes two inputs (A & B) and outputs *True* if **only both outputs are the same*. Otherwise outputs *False*.\n",
        "    * This is the opposite of the XOR Gate, effectivly as if you take the output of an XOR gate and pass it through a NOT gate\n",
        "<center><img src=\"img/logic_gates.png\" alt=\"Logic gate symbols, as typically used in electronics\"/></center>\n",
        "\n",
        "It's common to compile these operations into a \"truth table\", like below. Columns $A$ and $B$ represent the inputs, and the name of the *Boolean Logic Gates* represents the output for the given values of $A$ and $B$ in a row.\n",
        "\n",
        "| $A$ | $B$ | AND | OR | NOT* | NAND | NOR | XOR | XNOR |\n",
        "| :-: | :-: | :---: | :--: | :---: | :----: | :---: | :---: | :----: |\n",
        "| 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 0 |\n",
        "| 1 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
        "\n",
        "\\* Only uses one input, column A\n",
        "\n",
        "\n",
        "Defining $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and their inverted counterparts.\n",
        "\n",
        "Because they're only one layer, they're unable to represent logical compounds such as XOR/XNOR.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdFG8U2Net8"
      },
      "source": [
        "## Using numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug4zqMhLB-B6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea5cM4JEsENr"
      },
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a **weight** (vector) ; $b \\in \\mathbb{R}$ is a **bias** (scalar) ; and $g(.)$ denotes an **activation function** (in this case, that's the Unit Step Function) (we assume $g(0)=0$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRpaNDA8BWJY"
      },
      "source": [
        "For simplicity, let us consider examples with two-dimensional inputs ($d=2$).\n",
        "We can represent an input vector $\\boldsymbol{x} \\in \\mathbb{R}^2$ and weight vector $\\boldsymbol{w} \\in \\mathbb{R}^2$ with `numpy.array`. We also define the bias term $b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyX1MfRvBD25"
      },
      "outputs": [],
      "source": [
        "# Define some weight, input, and bias values to use\n",
        "x = np.array([0, 1]) # 3 dimensional vector with possible values {0,1} - Inputs\n",
        "w = np.array([1.0, 1.0]) # 2 dimensional vector with possible values ‚Ñù    - Weights\n",
        "b = 1.0 # scalar value                                                    - Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-P2AkbTCI0P"
      },
      "source": [
        "The following code computes $\\boldsymbol{w} \\cdot \\boldsymbol{x} + b$,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g_115nLCGqs"
      },
      "outputs": [],
      "source": [
        " np.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzhy8YFDIuf"
      },
      "source": [
        "We can apply the step function (also known as a Heaviside or Unit step function) as an *activation function*, $g()$:\n",
        "\n",
        "<img src=\"img/heaviside_step.png\" alt=\"Plot of the Heaviside Step Function\" style=\"background-color:white; width: 400px;\"/>\n",
        "When applied to the above result as $g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b)$, it yields a binary label, $\\hat{y}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV6a5JT5DH6c"
      },
      "outputs": [],
      "source": [
        "np.heaviside(np.dot(x, w) + b, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ppo_Jh8EEs3"
      },
      "source": [
        "#### Including the bias term into the weight vector\n",
        "\n",
        "For concise implementation, we include a bias term `b` as an additional dimension to the weight vector `w`. More concretely, we append an element with the value of $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x} = (0, 1) \\rightarrow \\boldsymbol{x}' = (0, 1, 1)\n",
        "$$\n",
        "and expand the dimension of the weight vector $\\boldsymbol{w} \\in \\mathbb{R}^{3}$.\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "In other words, $w_1$ and $w_2$ represent weights for $x_1$ and $x_2$, respectively, and $w_3$ is our bias value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmQGDVM2FnGy"
      },
      "outputs": [],
      "source": [
        "x = np.array([0, 1, 1])\n",
        "w = np.array([1.0, 1.0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53zn1dWQFvLw"
      },
      "source": [
        "We can simplify the code to predict a binary label $\\hat{y}$,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsgq_oOzF4PZ"
      },
      "outputs": [],
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SDTFENBA19"
      },
      "source": [
        "#### Training a NAND gate\n",
        "\n",
        "Let's train a NAND gate with two inputs. More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias value $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "| :---: |:---: | :---: |\n",
        "| 0 | 0 | 1 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "As explained earlier, we include the bias term into the last dimension.\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (50 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ygoUjQYrPoj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1., 1., 1., 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "lr = 0.5 #Learning Rate\n",
        "for t in range(50):\n",
        "    # Pick one random sample of traing data, at index (i), at random.\n",
        "    i = random.choice(range(len(y)))\n",
        "    # Predict the label for the instance x[i] with the current parameter w.\n",
        "    y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "    # Show the detail of the instance and the current parameter.\n",
        "    print(f'#{t:<2}: training sample index={i}, x={x[i]}, w={w}, y={y[i]}, y_pred={y_pred}, y_err={y[i] - y_pred}')\n",
        "    # Update the parameter.\n",
        "    loss = y[i] - y_pred\n",
        "    w += loss * lr * x[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8geTVVpnlu1O"
      },
      "source": [
        "We can confirm the learned parameter and classification results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYoeshu2rXdK"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOFgUFojraFA"
      },
      "outputs": [],
      "source": [
        "y_pred=np.heaviside(np.dot(x, w), 0)\n",
        "print(f\" Truth: {y}\")\n",
        "print(f\"  Pred: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl_ZAEguNzgU"
      },
      "source": [
        "### Single-layer perceptron with batching\n",
        "\n",
        "When training a model using a larger dataset with many samples, it's ideal to reduce the number of computations you perform - especially when running native python code (which is a lot slower than other languages). The common technique to speed up a machine-learning code written in Python is to run code using libraries that accelerate the large matrix operations, such as numpy (or Tensorflow, Keras, and Pytorch, but we'll get to that in a bit!)\n",
        "\n",
        "Even when using a libaray to accelerate these operations, we'll still run into issues with the sheer number of computations if we calculated the loss and updated the weights for every single image, so instead we use a technique called **batching**, where we calculate the loss of multiple images at a time and update the weights once every $n$ images, where $n$ is the number of images in one batch, or the **batch size**\n",
        "\n",
        "So, putting that into practice;\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "and if we give it 4 inputs, we'll get an output for each input\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix}\n",
        "  \\hat{y}_1 \\\\\n",
        "  \\hat{y}_2 \\\\\n",
        "  \\hat{y}_3 \\\\\n",
        "  \\hat{y}_4 \\\\\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix}\n",
        "  \\hat{y}_1 \\\\\n",
        "  \\hat{y}_2 \\\\\n",
        "  \\hat{y}_3 \\\\\n",
        "  \\hat{y}_4 \\\\\n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix}\n",
        "  \\boldsymbol{x}_1 \\\\\n",
        "  \\boldsymbol{x}_2 \\\\\n",
        "  \\boldsymbol{x}_3 \\\\\n",
        "  \\boldsymbol{x}_4 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD7f0rdz4wxS"
      },
      "outputs": [],
      "source": [
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "w = np.array([1.0, 0.5, -0.5])\n",
        "np.heaviside(np.dot(x, w), 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfwL6Z8q5Cyu"
      },
      "source": [
        "The code below applies the Perceptron algorithm with batching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fK-_WimtPwb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "lr = 0.5\n",
        "for t in range(10):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0) # Instead of picking a single image to calculate with, we give it 4 at once\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    Yerr = (y - y_pred) #loss is now a 4 dimensional vector - one value for each input from x\n",
        "    w += lr * np.dot(Yerr, x) # update the weight parameters,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvR709WnUlH"
      },
      "source": [
        "We can confirm the learned parameters and classification results match what we expect/the example done without batching, but this time we've only updated the weights 10 times vs 50 times!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x4p1BldtQ-K",
        "tags": []
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-P2RpWrtVyf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y_pred=np.heaviside(np.dot(x, w), 0)\n",
        "print(f\" Truth: {y}\")\n",
        "print(f\"  Pred: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxBcCSTtDvm"
      },
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch\n",
        "\n",
        "SGD works by updating the weights an iteration at a time using the function -\n",
        "$$ i=1, 2, ..., n $$\n",
        "$$ w_{n+1} = w_{n} -\\eta \\nabla Q_i(w)$$\n",
        "\n",
        "(note that $\\eta$, the greek letter \"Eta\" pronounced \"e-ta\", is different than $n$)!\n",
        "\n",
        "This is where $w$ is the weight, $\\eta$ is the rate of update (or the learning rate),$n$ is the batch size, and $\\nabla J(w)$ is the gradient of a batch of data's loss.\n",
        "\n",
        "The weights are updated with each batch of loss, $Q_i(w)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bltpfNRctjV5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={y-y_pred}, dw={np.dot((y - y_pred), x)}')\n",
        "    w -= eta * np.dot((y_pred - y), x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bASDMfhtm-I"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-69r0c4KtqlW"
      },
      "outputs": [],
      "source": [
        "sigmoid(np.dot(x, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK6zfKzwXW73"
      },
      "source": [
        "## Challenge - Write a different loss function\n",
        "\n",
        "The above code updates the model for a specific loss function, $\\hat y - y$. Modify the code below to use a loss function that measures the absolute squared difference between the true and predicted valued!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrGQrhNXV3f"
      },
      "outputs": [],
      "source": [
        "def new_loss(y_pred, y):\n",
        "    loss = None\n",
        "    #write your code here!\n",
        "    return loss\n",
        "\n",
        "\n",
        "eta = 0.5\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "for t in range(25):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={new_loss(y_pred, y)}, dw={np.dot(new_loss(y_pred, y), x)}')\n",
        "    #Define new_loss() above to calcualte the new loss equation!\n",
        "    w -= eta * np.dot(new_loss(y_pred, y), x)\n",
        "sigmoid(np.dot(x, w))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def new_loss(y_pred, y):\n",
        "    loss = None\n",
        "    loss = np.abs(y_pred-y)**2\n",
        "    return loss\n",
        "\n",
        "\n",
        "eta = 0.5\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "for t in range(25):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    print(f'#{t}: w={w}, Y={y}, Ypred={y_pred}, Yerr={new_loss(y_pred, y)}, dw={np.dot(new_loss(y_pred, y), x)}')\n",
        "    #Define new_loss() above to calcualte the new loss equation!\n",
        "    w -= eta * np.dot(new_loss(y_pred, y), x)\n",
        "sigmoid(np.dot(x, w))"
      ],
      "metadata": {
        "id": "h1dxGJ2fbttu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if your loss worked!\n",
        "test_pred = np.array([0.75415178, 0.67352346, 0.67352346, 0.58113885])\n",
        "test_y = np.array([1,1,1,0])\n",
        "expected_loss = np.array([0.06044135, 0.10658693, 0.10658693, 0.33772236])\n",
        "test_loss = new_loss(test_pred, test_y)\n",
        "print(\"These should both match!\")\n",
        "print(expected_loss)\n",
        "print(test_loss)"
      ],
      "metadata": {
        "id": "s-9VxqQS-Aaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGS23bDSazMJ"
      },
      "source": [
        "### Single-layer neural network with high-level Deep Learning Libraries\n",
        "\n",
        "So far, we've mostly used Numpy and sklearn and written the code for everything, but as you might've noticed, this can get somewhat tedious and repetitive!\n",
        "\n",
        "Thankfully, some very talented people have decided to make our jobs easier and write libraries that do a lot of the hard stuff for us, and even run faster than any code we could write in native python!\n",
        "\n",
        "Here, we'll show you how to do some basic operations in **TensorFlow/Keras**, but there are other libraries out there (Namely **PyTorch**!) that are just as powerful and useful for various use cases and applications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q-zWy4TLtUB"
      },
      "outputs": [],
      "source": [
        "# Import a bunch of TensorFlow stuff for now and later...\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2, l1\n",
        "from tensorflow.keras.losses import BinaryCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX6K-CT3LtUC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# wrap our input data in a datatype tensorflow understands\n",
        "x = tf.Variable([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
        "y = tf.Variable([[1], [1], [1], [0]], dtype=tf.float32)\n",
        "\n",
        "eta = 0.5\n",
        "\n",
        "# Define that the input is 2 dimensions long\n",
        "input_layer = Input(shape=(2,))\n",
        "\n",
        "# Define the output is a single dimension\n",
        "dense_layer = Dense(1)(input_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=dense_layer)\n",
        "\n",
        "loss_function = BinaryCrossentropy()\n",
        "loss_history = []\n",
        "\n",
        "for t in range(10):                    # Iterate over 100 batches of data\n",
        "    trainable_variables = model.trainable_variables\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "\n",
        "        y_pred = model(x)                   # Make predictions.\n",
        "        loss = loss_function(y_pred, y)     # Compute the loss.\n",
        "        loss_history.append(loss)           # Record the loss value.\n",
        "\n",
        "        # Calculate gradients with respect to every trainable variable\n",
        "        grad = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "        for param_number, gradient in enumerate(grad):\n",
        "            model.trainable_variables[param_number].assign_sub(eta * gradient)   # Update the parameters using SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwXFLW6MN8AD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(loss_history)),loss_history)\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sABi6XbOLXE"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgs8e4d2LtUC"
      },
      "source": [
        "## The Easy Method - Challenge!\n",
        "\n",
        "Keras can hide the training loops by specifying the loss and optimizer to the `compile()` method. In a lot of cases, this is the way most people will use Keras (as long as they're not doing anything too weird!)\n",
        "\n",
        "Try training for 10 epochs, with a batch size of 2\n",
        "\n",
        "Use this version to train a model and plot the history.\n",
        "The syntatx for the training loop is [found here](https://keras.io/api/models/model_training_apis/#fit-method).\n",
        "\n",
        "### Note! You might get different values than your neighbor and between runs, this is expected, as the inital weight values are random every time you create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZyYVRSCLtUC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define that the input is 2 dimensions long\n",
        "input_layer = \"?\"\n",
        "\n",
        "# Define the output is a single dimension\n",
        "dense_layer = \"?\"\n",
        "\n",
        "model = Model(input_layer, dense_layer)\n",
        "model.compile(optimizer=\"SGD\", loss=\"mse\") # Use the automatic training loop to update the parameters\n",
        "\n",
        "\n",
        "history = model.fit(\"?????\").history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define that the input is 2 dimensions long\n",
        "input_layer = Input(shape=(2,))\n",
        "\n",
        "# Define the output is a single dimension\n",
        "dense_layer = Dense(1)(input_layer)\n",
        "\n",
        "model = Model(input_layer, dense_layer)\n",
        "model.compile(optimizer=\"SGD\", loss=\"mse\") # Use the automatic training loop to update the parameters\n",
        "\n",
        "history = model.fit(x,y,epochs=20,batch_size=2).history"
      ],
      "metadata": {
        "id": "GU4wYSaKOwaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D20KVsRJ5wJK"
      },
      "outputs": [],
      "source": [
        "#loss_history = history[\"??\"]\n",
        "loss_history = history[\"loss\"]\n",
        "plt.plot(range(len(loss_history)),loss_history)\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfuoJMeqbClA"
      },
      "source": [
        "# Multi-layer neural network with high-level NN modules\n",
        "\n",
        "So, these neurons are pretty neat! But, we're somewhat limited in the state we've been using them up until now...\n",
        "\n",
        "Why not add some more? and why not chain them on top of eachother?\n",
        "\n",
        "Here, we're going to look at a **Multi Layer Perceptron**, which, as the name implies, is very similar to the perceptron we've been using, but this time with multiple **layers**!\n",
        "\n",
        "**Layers** are what make a neural network into a **Deep Neural Network**, and officially pulls us into the realm of **Deep Learning** :)\n",
        "\n",
        "Multiple layers in a network allows for a model to learn more complex relationships, make more detailed distinctions, and easily handle tasks involving multiple classes!\n",
        "\n",
        "## Particle Physics Example: Jet Tagging\n",
        "\n",
        "<img src=\"img/jet_tagger_jets.png\" alt=\"2D Representations of the different kinds of particle jets the neural network will classify\" style=\"background-color:white; width: 800px;\"/>\n",
        "In this example, we'll be looking at a \"simple\" high energy physics task - **Jet Tagging**\n",
        "\n",
        "In a particle collider, Jets are sprays of hadrons produced in very high-energy particle collisions. We're interested in determining what kind of particle produced a given jet. As the above image shows, we have 5 different kinds of jets that we want to classify (tag).\n",
        "\n",
        "We have 16 **features**, that we have measured and/or calculated from collisions in the Large Hadron Collider that we will feed into our neural network, asking it to guess which one of the 5 possible particles was responsible for a given jet.  \n",
        "\n",
        "### Multi-Layer Perceptron - Your first Deep Neural Network!\n",
        "\n",
        "<img src=\"img/jet_tagger_mlp.png\" alt=\"Graph of the Jet Tagger MLP Neural Network\" style=\"background-color:white; width: 400px;\"/>\n",
        "\n",
        "The neural network that we're going to use to accomplish this is what we call a 3 **hidden layer** network, meaning there are 3 layers of neurons \"hidden\" between the input and the output of the network.\n",
        "\n",
        "But, we still have some work to do before building and training our network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU2G2uffLtUD"
      },
      "outputs": [],
      "source": [
        "#Import some tensorflow tools we need\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxsTZfzXLtUE"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "data = jet_tagging_data\n",
        "X, y = data['data'], data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfP4G9WNLtUE"
      },
      "outputs": [],
      "source": [
        "# look at some example data\n",
        "print(data['feature_names'])\n",
        "print(X.shape, y.shape)\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8i_17fRLtUE"
      },
      "source": [
        "## Data pre-processing\n",
        "As it stands, our data isn't quite ready to feed into a network. We need to do a little bit of work ahead of time (**preprocessing**) to format the data and apply some statistical methods to make things easier for the network to understand.\n",
        "\n",
        "### Scaling Data\n",
        "\n",
        "When training a model, the model needs to understand what things mean physically. Unfortunately, all they model knows is numbers. Because of this, a variable that has a very large scale will get undue importance according to the model. Because of this, we can scale the input to a range that is consistent between all the variables in the model.\n",
        "In this scale, we'll do it by scaling according to the mean and standard deviation of the variable; using a `StandardScaler`\n",
        "\n",
        "The standard scaler transforms variables according to the formula:\n",
        "\n",
        "$$ X_{scaled} = \\frac{(X_{original} - \\mu)}{ \\sigma }$$\n",
        "\n",
        "### One-Hot Encoding\n",
        "\n",
        "Models cannot natively understand strings (words, letters, etc), only numbers. Because of this, we need to give them a little bit of help by encoding labels. When labels do not have any sort of order, we transform them using one-hot encoding.\n",
        "\n",
        "A set of labels ` labels = ['a', 'b', 'c']` will be transformed into the binary logic -\n",
        "\n",
        "| | $a$ | $b$ | $c$ |\n",
        "| :---: | :---: |:---: | :---: |\n",
        "|1 | 1 | 0 | 0 |\n",
        "|2 | 0 | 1 | 0 |\n",
        "|3 | 0 | 0 | 1 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTtc9DaTLtUE"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y, 5)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBs24vTLLtUG"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_val = scaler.fit_transform(X_train_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1sDiq8DLtUG"
      },
      "source": [
        "## Writing our MLP in Keras\n",
        "\n",
        "We'll define our MLP using a `functional` api. This means we'll define the model layer by layer, and connect them together.\n",
        "\n",
        "The model is built out of 3 dense layers with an activation between each of them. Here, we're using an activation function called **ReLU**, which stands for **Rectified Linear Unit**, and is generally one of the more \"standard\" activation functions you'll see used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjHOzZcXLtUG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0M-BqfKLtUG"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_layer = Input(shape=(16,), name='input')\n",
        "x = Dense(64,name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(input_layer)\n",
        "x = ReLU()(x)\n",
        "x = Dense(32,name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001))(x)\n",
        "output = Softmax()(x)\n",
        "\n",
        "model = Model(input_layer, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRk3SAhJVOIp"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIzFc1MRLtUG"
      },
      "source": [
        "## Training our MLP\n",
        "\n",
        "To train the model, we define the training optimizer (how the weights will be updated, instead of using SDG, we can pick anything), the loss being used (this is a classifier model, so we'll use a classifier loss), and the data being used to train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6ss25zA9nPk"
      },
      "outputs": [],
      "source": [
        "# Compile our model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAdWR6o-LtUG"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "history = model.fit(\n",
        "    X_train_val, # Input data\n",
        "    y_train_val, # Truth Output data\n",
        "    batch_size=1024, # Batch Size\n",
        "    epochs=30, # How many times will we iterate over the whole training dataset to train the model?\n",
        "    validation_split=0.25, # How much of the train dataset do we want to reserve as our validation split?\n",
        "    shuffle=True).history #Do we want to to order of our training samples to be shuffled?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_6Hle_e6IXM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_history = history['loss']\n",
        "epochs = range(len(loss_history))\n",
        "#print(loss_history)\n",
        "plt.plot(epochs, loss_history)\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = history['val_loss']\n",
        "epochs = range(len(loss_history))\n",
        "plt.plot(epochs, loss_history)\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')"
      ],
      "metadata": {
        "id": "yUdsW0RtVYSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check model performance\n",
        "\n",
        "Now that we've trained our model, we need to check and verify how well it does!\n",
        "To do this, we first will be calculating all these performance numbers based on a **Test Set** of data we specifically held out from the training dataset.\n",
        "\n",
        "We can, and should, use multiple different metrics to evaluate how well our model does. **One metric is not enough to sufficiently represent how our model does!!**\n",
        "\n",
        "Specifically, we're going to look at:\n",
        "\n",
        "*   **Accuracy**: The overall ratio of correct guesses/total guesses. 100% is perfect, and 100/number of classes (so in our case, 100/5 classes = 20%) is \"worst case\", as it's statistically no better than picking a label at random.\n",
        "*   **Confusion Matrix**: This is a specific kind of plot that shows how many samples of a given class were predicted as what, for every class. The axis of this plot are \"True\" and \"Predicted\", usually showing the number or percentage of the samples guessed as such in each cell of the plot. This shows a more detailed breakdown of what the model is guessing, and where it might be confusing two or more classes.\n",
        "*   **Receiver Operating Characteristic curve aka ROC curve**:\n",
        "*   **(ROC) Area Under the Curve, aka (ROC)AUC**: This is simply a way to distill the ROC into a single value, and as the name implies, is effectively just taking the integral of the ROC curve. 100% is the best case scenerio, and 50% is the worst case scenerio.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5xr7PoRoVAjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti0_Pi56LtUH"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.metrics import auc, roc_curve, accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    # plt.title(title)\n",
        "    cbar = plt.colorbar()\n",
        "    plt.clim(0, 1)\n",
        "    cbar.set_label(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def plotRoc(fpr, tpr, auc, labels, linestyle, legend=True):\n",
        "    for _i, label in enumerate(labels):\n",
        "        plt.plot(\n",
        "            tpr[label],\n",
        "            fpr[label],\n",
        "            label='{} tagger, AUC = {:.1f}%'.format(label.replace('j_', ''), auc[label] * 100.0),\n",
        "            linestyle=linestyle,\n",
        "        )\n",
        "    plt.semilogy()\n",
        "    plt.xlabel(\"Signal Efficiency\")\n",
        "    plt.ylabel(\"Background Efficiency\")\n",
        "    plt.ylim(0.001, 1)\n",
        "    plt.grid(True)\n",
        "    if legend:\n",
        "        plt.legend(loc='upper left')\n",
        "    plt.figtext(0.25, 0.90, 'Jet Tagging MLP Performance', fontweight='bold', wrap=True, horizontalalignment='right', fontsize=14)\n",
        "\n",
        "\n",
        "def rocData(y, predict_test, labels):\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    auc1 = {}\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        df[label] = y[:, i]\n",
        "        df[label + '_pred'] = predict_test[:, i]\n",
        "\n",
        "        fpr[label], tpr[label], threshold = roc_curve(df[label], df[label + '_pred'])\n",
        "\n",
        "        auc1[label] = auc(fpr[label], tpr[label])\n",
        "    return fpr, tpr, auc1\n",
        "\n",
        "\n",
        "def makeRoc(y, predict_test, labels, linestyle='-', legend=True):\n",
        "    if 'j_index' in labels:\n",
        "        labels.remove('j_index')\n",
        "\n",
        "    fpr, tpr, auc1 = rocData(y, predict_test, labels)\n",
        "    plotRoc(fpr, tpr, auc1, labels, linestyle, legend=legend)\n",
        "    return predict_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BF6-W_J-82M"
      },
      "outputs": [],
      "source": [
        "y_keras = model.predict(X_test)\n",
        "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
        "plt.figure(figsize=(9, 9))\n",
        "_ = makeRoc(y_test, y_keras, le.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_cm = np.argmax(y_keras, axis=1)\n",
        "y_cm_true = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_cm_true, y_cm)\n",
        "plot_confusion_matrix(cm, le.classes_, normalize=True)"
      ],
      "metadata": {
        "id": "UGjmDWHcYM6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlgDqOntYNGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3K-JHWLtUH"
      },
      "source": [
        "## Attribution, Sources, and Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4kLg0dTLtUH"
      },
      "source": [
        "This notebook was derived from two, seperate notebooks from the Tokyo Institute of Technology's ART.T458: Advanced Machine Learning course, and were originally authored by Prof. Naoaki Okazaki.\n",
        "\n",
        "The original notebooks (and accompanying material) can be found here: https://chokkan.github.io/deeplearning/\n",
        "\n",
        "The Jet Tagging MLP portion was adapted from [the hls4ml tutorial part 1 notebook](https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part1_getting_started.ipynb)\n",
        "\n",
        "Modifications to this notebook were done by Ben Hawks for the 2023 Fermilab and Brookhaven National Lab Summer Exchange School\n",
        "\n",
        "Various sources for images and other materials used is listed below:\n",
        "1. [Matrix Transpose Gif by Lucas Vieira via Wikipedia](https://commons.wikimedia.org/wiki/File:Matrix_transpose.gif)\n",
        "3. [Boolean Logic Gates Image/Symbols (Digilent) ](https://digilent.com/blog/logic-gates/)\n",
        "    * Original/Individual Symbols used (IEEE Std 91/91a-1991 \"Distinctive Shapes\") are originally via [Inductiveload via Wikipedia](https://en.wikipedia.org/wiki/Logic_gate#Symbols)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Jbloif7U2s_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}